{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating Features Regarding Descriptions by Merchant\n",
    "## Author: Dulan Wijeratne 1181873"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will aggregate the features regarding descriptions by merchant abn, as well as group merchants into segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we will create a Spark session and import the descriptions dataset that contains all the features that relate to descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/10/03 20:43:47 WARN Utils: Your hostname, DulanComputer resolves to a loopback address: 127.0.1.1; using 172.30.15.25 instead (on interface eth0)\n",
      "23/10/03 20:43:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/03 20:43:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Preprocessing_Yellow\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.driver.memory', '3g')   \n",
    "    .config('spark.executor.memory', '4g')  \n",
    "    .config('spark.executor.instances', '2')  \n",
    "    .config('spark.executor.cores', '2')\n",
    "    .getOrCreate()\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "descriptions = spark.read.parquet(\"../../../data/insights/pre_insights/descriptions.parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to check whether each merchant has a multiple descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tags_count = descriptions.groupBy(\"merchant_abn\").agg(f.countDistinct(\"merchant_description\").alias(\"description_count\"))\n",
    "print(tags_count.filter(tags_count.description_count > 1).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "Here we will aggregate the data by merchant abn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_agg = descriptions.groupBy(\"merchant_abn\").\\\n",
    "            agg(f.first(\"merchant_description\").alias(\"merchant_description\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenting the data\n",
    "\n",
    "Here we will segment the data by descriptions into 5 groups:\n",
    "1. Tech and Electronics - Contains anything to do with computers, digital good, telecom and etc.\n",
    "2. Retail and Novelty - Contains anything found in a department store or a novelty store\n",
    "3. Garden and Furnishings - Contains anything to do with furniture and gardening including florist and tents \n",
    "4. Antiques and Jewellery - Contains anything to do with jewellery, galleries and antiques.\n",
    "5. Specialized Services - Contains speacialized services such as opticians, motor vehecle services and health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to check how many unique descriptions there are and get an idea of the number of merchants within each description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions.groupBy(\"merchant_description\").agg(f.first(\"merchant_abn\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_count = descriptions_agg.groupBy(\"merchant_description\").agg(f.count(\"merchant_abn\").alias(\"number_of_merchants_with_description\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_count.orderBy(\"number_of_merchants_with_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now will begin segmenting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists which contain keywords for each segment\n",
    "tech_and_electronics = [\"computer\", \"digital\", \"television\", \"telecom\"]\n",
    "retail_and_novelty = [\"newspaper\", \"novelty\", \"hobby\", \"shoe\", \"instruments\", \"bicycle\", \"craft\",\"office\"]\n",
    "garden_and_furnishings = [\"florists\", \"furniture\", \"garden\", \"tent\"]\n",
    "antiques_and_jewellery = [\"galleries\", \"antique\", \"jewelry\"]\n",
    "specialized_services = [\"health\", \"motor\", \"opticians\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that segments the data if the description contains a keyword\n",
    "def segment(description):\n",
    "    for segment, keywords in [(\"tech_and_electronics\", tech_and_electronics),\n",
    "                               (\"retail_and_novelty\", retail_and_novelty),\n",
    "                               (\"garden_and_furnishings\", garden_and_furnishings),\n",
    "                               (\"antiques_and_jewellery\", antiques_and_jewellery),\n",
    "                               (\"specialized_services\", specialized_services)]:\n",
    "        if any(keyword in description for keyword in keywords):\n",
    "            return segment\n",
    "    return \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "segment_udf = f.udf(segment, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_agg = descriptions_agg.withColumn(\"segment\", segment_udf(descriptions_agg.merchant_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking whether all merchants were put in a segment\n",
    "other_df = descriptions_agg.filter(f.col(\"segment\") == \"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------+\n",
      "|merchant_abn|merchant_description|segment|\n",
      "+------------+--------------------+-------+\n",
      "+------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "other_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_agg.write.mode(\"overwrite\").parquet(\"../../../data/insights/agg_insight_data/descriptions_agg.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Each merchant has only one description\n",
    "- Data was aggregated by merchant abn:\n",
    "    1. Merchant description was aggregation using the first function as it is unique for each merchant.\n",
    "\n",
    "- There are only 25 different merchant.\n",
    "- Merchants were put into 1 of 5 segments:\n",
    "    1. Tech and Electronics\n",
    "    2. Retail and Novelty\n",
    "    3. Garden and Furnishings\n",
    "    4. Antiques and Jewellery\n",
    "    5. Specialized Services\n",
    "    \n",
    "- Aggregated data was saved to a checkpoint directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
