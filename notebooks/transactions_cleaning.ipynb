{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/09/02 16:11:11 WARN Utils: Your hostname, NivethanAsus resolves to a loopback address: 127.0.1.1; using 172.24.38.5 instead (on interface eth0)\n",
      "23/09/02 16:11:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/02 16:11:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, to_date, when, concat, lit\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"green preprocessing\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------------+--------------------+--------------+\n",
      "|user_id|merchant_abn|      dollar_value|            order_id|order_datetime|\n",
      "+-------+------------+------------------+--------------------+--------------+\n",
      "|  18478| 62191208634|63.255848959735246|949a63c8-29f7-4ab...|    2021-08-20|\n",
      "|      2| 15549624934| 130.3505283105634|6a84c3cf-612a-457...|    2021-08-20|\n",
      "|  18479| 64403598239|120.15860593212783|b10dcc33-e53f-425...|    2021-08-20|\n",
      "|      3| 60956456424| 136.6785200286976|0f09c5a5-784e-447...|    2021-08-20|\n",
      "|  18479| 94493496784| 72.96316578355305|f6c78c1a-4600-4c5...|    2021-08-20|\n",
      "+-------+------------+------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==================================================>      (23 + 3) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- dollar_value: double (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path1 = \"../data/tables/transactions_20210228_20210827_snapshot/\"\n",
    "path2 = \"../data/tables/transactions_20210828_20220227_snapshot/\"\n",
    "path3 = \"../data/tables/transactions_20220228_20220828_snapshot/\"\n",
    "df_1 = spark.read.parquet(path1)\n",
    "df_2 = spark.read.parquet(path2)\n",
    "df_3 = spark.read.parquet(path3)\n",
    "df = df_1.union(df_2).union(df_3)\n",
    "\n",
    "df.show(5)\n",
    "df.count()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Counts:  {'user_id': 0, 'merchant_abn': 0, 'dollar_value': 0, 'order_id': 0, 'order_datetime': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Count:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid User/Merchant Count:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Dollar Value Count:  0\n",
      "Invalid Date Count:  10552239\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Check for Null Values\n",
    "null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "print(\"Null Counts: \", null_counts)\n",
    "\n",
    "# 2. Check for Duplicates\n",
    "duplicate_count = df.count() - df.dropDuplicates().count()\n",
    "print(\"Duplicate Count: \", duplicate_count)\n",
    "\n",
    "# 3. Validate user_id and merchant_abn\n",
    "invalid_user_merchant_count = df.filter((F.col('user_id') <= 0) | (F.col('merchant_abn') <= 0)).count()\n",
    "print(\"Invalid User/Merchant Count: \", invalid_user_merchant_count)\n",
    "\n",
    "# 4. Validate dollar_value\n",
    "invalid_dollar_value_count = df.filter(F.col('dollar_value') <= 0).count()\n",
    "print(\"Invalid Dollar Value Count: \", invalid_dollar_value_count)\n",
    "\n",
    "\n",
    "# 6. Validate order_datetime for the specified date range\n",
    "start_date = datetime.strptime(\"20210228\", \"%Y%m%d\").date()\n",
    "end_date = datetime.strptime(\"20210827\", \"%Y%m%d\").date()\n",
    "invalid_date_count = df.filter(~F.col('order_datetime').between(start_date, end_date)).count()\n",
    "print(\"Invalid Date Count: \", invalid_date_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"../data/transactions_cleaned.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
